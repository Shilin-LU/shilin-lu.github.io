---
---

@article{lu2022copy,
  title={Copy-move image forgery detection based on evolving circular domains coverage},
  author={Lu, Shilin and Hu, Xinghong and Wang, Chengyou and Chen, Lu and Han, Shulu and Han, Yuejia},
  journal={Multimedia Tools and Applications},
  volume={81},
  number={26},
  pages={37847--37872},
  year={2022},
  publisher={Springer},
  abbr={MTA},
  arxiv={2109.04381},
  abstract={The aim of this paper is to improve the accuracy of copy-move forgery detection (CMFD) in image forensics by proposing a novel scheme and the main contribution is evolving circular domains coverage (ECDC) algorithm. The proposed scheme integrates both block-based and keypoint-based forgery detection methods. Firstly, the speed-up robust feature (SURF) in log-polar space and the scale invariant feature transform (SIFT) are extracted from an entire image. Secondly, generalized 2 nearest neighbor (g2NN) is employed to get massive matched pairs. Then, random sample consensus (RANSAC) algorithm is employed to filter out mismatched pairs, thus allowing rough localization of counterfeit areas. To present these forgery areas more accurately, we propose the efficient and accurate ECDC algorithm to present them. This algorithm can find satisfactory threshold areas by extracting block features from jointly evolving circular domains, which are centered on matched pairs. Finally, morphological operation is applied to refine the detected forgery areas. Experimental results indicate that the proposed CMFD scheme can achieve better detection performance under various attacks compared with other state-of-the-art CMFD schemes.},
  html={https://link.springer.com/article/10.1007/s11042-022-12755-w},
  selected={true},
  preview={ecdc.png}
}

@article{lu_2023_tficon,
  author    = {Lu, Shilin and Liu, Yanzhu and Kong, Adams Wai-Kin},
  title     = {TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition},
  journal = {IEEE/CVF International Conference on Computer Vision <b style="font-style: normal;">(ICCV)</b>},
  year      = {2023},
  arxiv={2307.12493},
  abstract={Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains.},
  proj_page={https://shilin-lu.github.io/tf-icon.github.io/},
  selected={true},
  code={https://github.com/Shilin-LU/TF-ICON},
  preview={tficon.png},
  abbr={ICCV 2023},
}

@article{lu2024mace,
  title={Mace: Mass concept erasure in diffusion models},
  author={Lu, Shilin and Wang, Zilan and Li, Leyang and Liu, Yanzhu and Kong, Adams Wai-Kin},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition <b style="font-style: normal;">(CVPR)</b>},
  pages={6430--6440},
  year={2024},
  abbr={CVPR 2024},
  selected={true},
  code={https://github.com/Shilin-LU/MACE},
  preview={mace.png},
  arxiv={2403.06135},
  abstract={The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks.},
}

@article{lu2025robust,
  title={Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances},
  author={Shilin Lu and Zihan Zhou and Jiayou Lu and Yuanzhi Zhu and Adams Wai-Kin Kong},
  journal={International Conference on Learning Representations <b style="font-style: normal;">(ICLR)</b>},
  year={2025},
  openreview={https://openreview.net/forum?id=16O8GCm8Wn},
  selected={true},
  code={https://github.com/Shilin-LU/VINE},
  preview={vine.png},
  arxiv={2410.18775},
  abstract={Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness.},
  abbr={ICLR 2025},
}

@article{gao2024eraseanything,
  title={EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers},
  author={Gao, Daiheng and Lu, Shilin and Walters, Shaw and Zhou, Wenbo and Chu, Jiaming and Zhang, Jie and Zhang, Bang and Jia, Mengxi and Zhao, Jian and Fan, Zhaoxin and Zhang, Weiming},
  journal={<span style="font-style: normal;">Preprint</span>},
  year={2024},
  selected={true},
  preview={eraseanything.png},
  arxiv={2412.20413},
  abstract={Removing unwanted concepts from large-scale text-to-image (T2I) diffusion models while maintaining their overall generative quality remains an open challenge. This difficulty is especially pronounced in emerging paradigms, such as Stable Diffusion (SD) v3 and Flux, which incorporate flow matching and transformer-based architectures. These advancements limit the transferability of existing concept-erasure techniques that were originally designed for the previous T2I paradigm (e.g., SD v1.4). In this work, we introduce EraseAnything, the first method specifically developed to address concept erasure within the latest flow-based T2I framework. We formulate concept erasure as a bi-level optimization problem, employing LoRA-based parameter tuning and an attention map regularizer to selectively suppress undesirable activations. Furthermore, we propose a self-contrastive learning strategy to ensure that removing unwanted concepts does not inadvertently harm performance on unrelated ones. Experimental results demonstrate that EraseAnything successfully fills the research gap left by earlier methods in this new T2I paradigm, achieving state-of-the-art performance across a wide range of concept erasure tasks.},
  abbr={Preprint},
}

@article{zhu2024oftsr,
  title={OFTSR: One-Step Flow for Image Super-Resolution with Tunable Fidelity-Realism Trade-offs},
  author={Zhu, Yuanzhi and Wang, Ruiqing and Lu, Shilin and Li, Junnan and Yan, Hanshu and Zhang, Kai},
  journal={<span style="font-style: normal;">Preprint</span>},
  year={2024},
  selected={true},
  preview={oftsr.png},
  arxiv={2412.09465},
  code={https://github.com/yuanzhi-zhu/OFTSR},
  abstract={Recent advances in diffusion and flow-based generative models have demonstrated remarkable success in image restoration tasks, achieving superior perceptual quality compared to traditional deep learning approaches. However, these methods either require numerous sampling steps to generate high-quality images, resulting in significant computational overhead, or rely on model distillation, which usually imposes a fixed fidelity-realism trade-off and thus lacks flexibility. In this paper, we introduce OFTSR, a novel flow-based framework for one-step image super-resolution that can produce outputs with tunable levels of fidelity and realism. Our approach first trains a conditional flow-based super-resolution model to serve as a teacher model. We then distill this teacher model by applying a specialized constraint. Specifically, we force the predictions from our one-step student model for same input to lie on the same sampling ODE trajectory of the teacher model. This alignment ensures that the student model's single-step predictions from initial states match the teacher's predictions from a closer intermediate state. Through extensive experiments on challenging datasets including FFHQ (256×256), DIV2K, and ImageNet (256×256), we demonstrate that OFTSR achieves state-of-the-art performance for one-step image super-resolution, while having the ability to flexibly tune the fidelity-realism trade-off.},
  abbr={Preprint},
}

@article{ren2025all,
  title={All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian Splatting},
  author={Ren, Yan and Lu, Shilin and Kong, Adams Wai-Kin},
  journal={<span style="font-style: normal;">Preprint</span>},
  year={2025},
  selected={true},
  preview={keyss.png},
  arxiv={2503.07191},
  code={https://github.com/RY-Paper/KeySS},
  abstract={Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene reconstruction, opening new possibilities for 3D steganography by hiding 3D secrets within 3D covers. The key challenge in steganography is ensuring imperceptibility while maintaining high-fidelity reconstruction. However, existing methods often suffer from detectability risks and utilize only suboptimal 3DGS features, limiting their full potential. We propose a novel end-to-end key-secured 3D steganography framework (KeySS) that jointly optimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our approach reveals that Gaussian features contribute unequally to secret hiding. The framework incorporates a key-controllable mechanism enabling multi-secret hiding and unauthorized access prevention, while systematically exploring optimal feature update to balance fidelity and security. To rigorously evaluate steganographic imperceptibility beyond conventional 2D metrics, we introduce 3D-Sinkhorn distance analysis, which quantifies distributional differences between original and steganographic Gaussian parameters in the representation space. Extensive experiments demonstrate that our method achieves state-of-the-art performance in both cover and secret reconstruction while maintaining high security levels, advancing the field of 3D steganography.},
  abbr={Preprint},
}
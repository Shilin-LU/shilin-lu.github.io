---
---

@article{lu2022copy,
  title={Copy-move image forgery detection based on evolving circular domains coverage},
  author={Lu, Shilin and Hu, Xinghong and Wang, Chengyou and Chen, Lu and Han, Shulu and Han, Yuejia},
  journal={Multimedia Tools and Applications},
  doi={10.1007/s11042-022-12755-w},
  volume={81},
  number={26},
  pages={37847--37872},
  year={2022},
  publisher={Springer},

  abbr={MTA},
  arxiv={2109.04381},
  bibtex_show={true},
  dimensions={true},
  pdf={ecdc.pdf},
  abstract={The aim of this paper is to improve the accuracy of copy-move forgery detection (CMFD) in image forensics by proposing a novel scheme and the main contribution is evolving circular domains coverage (ECDC) algorithm. The proposed scheme integrates both block-based and keypoint-based forgery detection methods. Firstly, the speed-up robust feature (SURF) in log-polar space and the scale invariant feature transform (SIFT) are extracted from an entire image. Secondly, generalized 2 nearest neighbor (g2NN) is employed to get massive matched pairs. Then, random sample consensus (RANSAC) algorithm is employed to filter out mismatched pairs, thus allowing rough localization of counterfeit areas. To present these forgery areas more accurately, we propose the efficient and accurate ECDC algorithm to present them. This algorithm can find satisfactory threshold areas by extracting block features from jointly evolving circular domains, which are centered on matched pairs. Finally, morphological operation is applied to refine the detected forgery areas. Experimental results indicate that the proposed CMFD scheme can achieve better detection performance under various attacks compared with other state-of-the-art CMFD schemes.},
  html={https://link.springer.com/article/10.1007/s11042-022-12755-w},
  selected={true}
}

@InProceedings{lu_2023_tficon,
  author    = {Lu, Shilin and Liu, Yanzhu and Kong, Adams Wai-Kin},
  title     = {TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2023},

  abbr={ICCV},
  arxiv={2307.12493},
  bibtex_show={true},
  dimensions={true},
  pdf={tficon.pdf},
  abstract={Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains.},
  proj_page={https://shilin-lu.github.io/tf-icon.github.io/},
  selected={true},
  code={https://github.com/Shilin-LU/TF-ICON}
}
